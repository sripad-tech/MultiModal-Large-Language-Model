import os
import argparse
import matplotlib.pyplot as plt
from PIL import Image
from comet_ml import Experiment
from comet_ml.integration.pytorch import log_model
from pytorch_lightning.loggers import TensorBoardLogger, CometLogger
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint
from torchmetrics import Accuracy, F1Score, Recall, Precision
from sklearn.model_selection import train_test_split
from models import imagebind_model
from models import lora as LoRA
from models.imagebind_model import ModalityType, load_module, save_module
import torch.optim as optim
import torch.nn.functional as F

from torch.utils.data import Sampler
import numpy as np
from itertools import cycle, islice

class BalancedModalityConditionSamplerWithRepetition(Sampler):
    def __init__(self, dataset, batch_size):
        self.dataset = dataset
        self.batch_size = batch_size
        assert batch_size % 4 == 0, "Batch size must be divisible by 4 for balance across two modalities and two conditions."

        self.indices = {('audio', 'ALZHEIMERS_DISEASE'): [], ('audio', 'COGNITIVE_NORMAL'): [],
                        ('vision', 'ALZHEIMERS_DISEASE'): [], ('vision', 'COGNITIVE_NORMAL'): []}

        for idx, (_, label, modality) in enumerate(dataset):
            condition = 'ALZHEIMERS_DISEASE' if label == 0 else 'COGNITIVE_NORMAL'
            modality = 'audio' if dataset.modalities[idx] == 'audio' else 'vision'
            self.indices[(modality, condition)].append(idx)

        # Here, calculate the min_group_size based on the lengths of the lists in self.indices
        self.min_group_size = min(len(indices) for indices in self.indices.values())

        # Now, convert indices to cycle objects for infinite cycling
        self.indices = {key: cycle(val) for key, val in self.indices.items()}

    def __iter__(self):
        group_batch_size = self.batch_size // 4
        max_batches = self.min_group_size // group_batch_size
        for _ in range(max_batches):
            batch_indices = []
            for key in self.indices.keys():
                batch_indices.extend(islice(self.indices[key], group_batch_size))
            yield batch_indices

    def __len__(self):
        # Using min_group_size and batch_size to calculate the total number of batches
        return (self.min_group_size // (self.batch_size // 4)) * 4



class AlzheimerDataset(Dataset):
    def __init__(self, image_paths, labels, modalities, transform=None):
        self.image_paths = image_paths
        self.labels = labels
        self.modalities = modalities
        self.transform = transform

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        image = Image.open(img_path).convert('RGB')
        label = self.labels[idx]
        modality = self.modalities[idx]
        if self.transform:
            image = self.transform(image)
        return image, torch.tensor(label, dtype=torch.long), modality

def load_dataset(root_dir):
    image_paths, labels, modalities = [], [], []
    label_dirs = {'ALZHEIMERS_DISEASE': 0, 'COGNITIVE_NORMAL': 1}
    for label_name, label in label_dirs.items():
        label_dir = os.path.join(root_dir, label_name)
        for img_file in os.listdir(label_dir):
            if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):
                image_paths.append(os.path.join(label_dir, img_file))
                labels.append(label)
                if 'adrso' in img_file.lower():
                    modalities.append('audio')
                else:
                    modalities.append('vision')
    return image_paths, labels, modalities

class MLPClassifier(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(MLPClassifier, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

class ImageBindTrain(pl.LightningModule):
    def __init__(self, lora_rank, lora_checkpoint_dir, num_classes=2, lr=1e-4, weight_decay=1e-4, lora=False, lora_layer_idxs=None, lora_modality_names=None, **kwargs):
        super().__init__()
        self.save_hyperparameters()
        self.model = imagebind_model.imagebind_huge(pretrained=True)
        if lora:
            for modality_preprocessor in self.model.modality_preprocessors.children():
                modality_preprocessor.requires_grad_(False)
            for modality_trunk in self.model.modality_trunks.children():
                modality_trunk.requires_grad_(False)
            self.model.modality_trunks.update(LoRA.apply_lora_modality_trunks(self.model.modality_trunks, rank=lora_rank, layer_idxs=lora_layer_idxs, modality_names=lora_modality_names))
            LoRA.load_lora_modality_trunks(self.model.modality_trunks, checkpoint_dir=lora_checkpoint_dir)
            load_module(self.model.modality_postprocessors, module_name="postprocessors", checkpoint_dir=lora_checkpoint_dir)
            load_module(self.model.modality_heads, module_name="heads", checkpoint_dir=lora_checkpoint_dir)
        elif linear_probing:
            for modality_preprocessor in self.model.modality_preprocessors.children():
                modality_preprocessor.requires_grad_(False)
            for modality_trunk in self.model.modality_trunks.children():
                modality_trunk.requires_grad_(False)
            for modality_postprocessor in self.model.modality_postprocessors.children():
                modality_postprocessor.requires_grad_(False)
            load_module(self.model.modality_heads, module_name="heads", checkpoint_dir=lora_checkpoint_dir)
            for modality_head in self.model.modality_heads.children():
                modality_head.requires_grad_(False)
            final_layer = list(modality_head.children())[-1]
            final_layer.requires_grad_(True)

        self.classifier = MLPClassifier(1024, 512, num_classes)
        self.accuracy = Accuracy(num_classes=num_classes, average='macro', task='multiclass')
        self.f1 = F1Score(num_classes=num_classes, average='macro', task='multiclass')
        self.recall = Recall(num_classes=num_classes, average='macro', task='multiclass')
        self.precision = Precision(num_classes=num_classes, average='macro', task='multiclass')
        self.criterion_classification = nn.CrossEntropyLoss()

    def forward(self, x, modality):
        inputs = {modality: x}
        features = self.model(inputs)
        output_features = features.get(modality)
        if output_features is not None:
            return self.classifier(output_features)
        else:
            raise ValueError("Model did not return features for the specified modality")

    def info_nce_loss(self, batch, mode="train"):
        images, labels, modalities = batch
        data_a = images[modalities == 'vision']
        class_a = ['vision'] * len(data_a)
        data_b = images[modalities == 'audio']
        class_b = ['audio'] * len(data_b)
		
        #min_samples = min(len(data_a), len(data_b))
        #data_a = data_a[:min_samples]
        #class_a = class_a[:min_samples]
        #data_b = data_b[:min_samples]
        #class_b = class_b[:min_samples]
        
        feats_a = [self.model({class_a[0]: data_a_i}) for data_a_i in data_a]
        feats_a_list = [list(dict_.values())[0] for dict_ in feats_a if dict_]

        feats_b = [self.model({class_b[idx]: data_b_i}) for idx, data_b_i in enumerate(data_b)]
        feats_b_list = [list(dict_.values())[0] for dict_ in feats_b if dict_]

    # Check if either feature list is empty and skip concatenation if so
        if not feats_a_list or not feats_b_list:
    # Handle the empty case, e.g., by returning a zero loss or a default value
    # Return zero loss and empty metrices: FallBack mechanism
            fallback_loss = torch.tensor(0.0, device=self.device, requires_grad=True)
            metrics = {"acc_top1": torch.tensor(0.0, device=self.device), 
                   "acc_top5": torch.tensor(0.0, device=self.device),
                   "acc_mean_pos": torch.tensor(0.0, device=self.device)}
            self.log(f"{mode}_loss", fallback_loss, on_step=False, on_epoch=True, prog_bar=True)
            for key, value in metrics.items():
                self.log(f"{mode}_{key}", value, on_step=False, on_epoch=True, prog_bar=True)
            return fallback_loss, metrics

        feats_a_tensor = torch.cat(feats_a_list, dim=0)
        feats_b_tensor = torch.cat(feats_b_list, dim=0)

        feats_a_b_tensor = torch.cat([feats_a_tensor, feats_b_tensor], dim=0)
        feats_tensors = [feats_a_b_tensor]
        temperatures = [self.hparams.temperature]
        contrast = ["cross"]

        dual_nll = False
        for feats_idx, feats_tensor in enumerate(feats_tensors):
            cos_sim = F.cosine_similarity(feats_tensor[:, None, :], feats_tensor[None, :, :],dim=-1)
            self_mask = torch.eye(cos_sim.shape[0], dtype=torch.bool, device=cos_sim.device)
            cos_sim.masked_fill_(self_mask, -9e15)
            pos_mask= self_mask.roll(shifts=cos_sim.shape[0] // 2, dims=0)
            cos_sim = cos_sim / temperatures[feats_idx]
            nll = -cos_sim[pos_mask] + torch.logsumexp(cos_sim, dim=-1)
            nll = nll.mean(keepdim=True)
            if not dual_nll:
                dual_nll = nll
            else:
                dual_nll += nll
                dual_nll /= 2
            self.log(mode + "_loss_" + contrast[feats_idx], nll, prog_bar=True,
                     on_step=False, on_epoch=True, batch_size=self.hparams.batch_size)
            comb_sim = torch.cat(
                [cos_sim[pos_mask][:, None], cos_sim.masked_fill(pos_mask, -9e15)],
                dim=-1,
            )
            sim_argsort = comb_sim.argsort(dim=-1, descending=True).argmin(dim=-1)
            self.log(mode + "_acc_top1", (sim_argsort == 0).float().mean(keepdim=True), prog_bar=True,
                     on_step=False, on_epoch=True, batch_size=self.hparams.batch_size)
            self.log(mode + "_acc_top5", (sim_argsort < 5).float().mean(keepdim=True), prog_bar=True,
                     on_step=False, on_epoch=True, batch_size=self.hparams.batch_size)
            self.log(mode + "_acc_mean_pos", 1 + sim_argsort.float().mean(keepdim=True), prog_bar=True,
                     on_step=False, on_epoch=True, batch_size=self.hparams.batch_size)
        self.log(mode + "_loss", dual_nll, prog_bar=True,
                 on_step=False, on_epoch=True, batch_size=self.hparams.batch_size)
        
        metrics = {
        "acc_top1": (sim_argsort == 0).float().mean(keepdim=True),
        "acc_top5": (sim_argsort < 5).float().mean(keepdim=True),
        "acc_mean_pos": 1 + sim_argsort.float().mean(keepdim=True),
        "loss": dual_nll
        }

        # Iterate over metrics dictionary and log each value
        for key, value in metrics.items():
            self.log(f"{mode}_{key}", value, prog_bar=True,
                 on_step=False, on_epoch=True, batch_size=self.hparams.batch_size)

        print(f"Debug info_nce_loss: loss shape: {dual_nll.shape}, metrics keys: {metrics.keys()}")

        return dual_nll, metrics

    def training_step(self, batch, batch_idx):
        images, labels, modalities = batch
        outputs = self(images, modalities[0])
        loss_classification = self.criterion_classification(outputs, labels)
        preds = torch.argmax(outputs, dim=1)
        acc = self.accuracy(preds, labels)
        f1 = self.f1(preds, labels)
        recall = self.recall(preds, labels)
        precision = self.precision(preds, labels)

        self.log('train_loss_classification', loss_classification, on_step=False, on_epoch=True, prog_bar=True)
        self.log('train_acc', acc, on_step=False, on_epoch=True, prog_bar=True)
        self.log('train_f1', f1, on_step=False, on_epoch=True, prog_bar=True)
        self.log('train_recall', recall, on_step=False, on_epoch=True, prog_bar=True)
        self.log('train_precision', precision, on_step=False, on_epoch=True, prog_bar=True)

        loss_retrieval = self.info_nce_loss(batch, mode="train")
        loss = loss_classification + loss_retrieval
        
        loss, metrics = self.info_nce_loss(batch, mode="train")
        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)
        for key, value in metrics.items():
            self.log(f"train_{key}", value, on_step=False, on_epoch=True, prog_bar=True)

        return loss

    def validation_step(self, batch, batch_idx):
        images, labels, modalities = batch
        outputs = self(images, modalities[0])
        loss_classification = self.criterion_classification(outputs, labels)
        preds = torch.argmax(outputs, dim=1)
        acc = self.accuracy(preds, labels)
        f1 = self.f1(preds, labels)
        recall = self.recall(preds, labels)
        precision = self.precision(preds, labels)

        self.log('val_loss_classification', loss_classification, on_step=False, on_epoch=True, prog_bar=True)
        self.log('val_acc', acc, on_step=False, on_epoch=True, prog_bar=True)
        self.log('val_f1', f1, on_step=False, on_epoch=True, prog_bar=True)
        self.log('val_recall', recall, on_step=False, on_epoch=True, prog_bar=True)
        self.log('val_precision', precision, on_step=False, on_epoch=True, prog_bar=True)

        loss, metrics = self.info_nce_loss(batch, mode="val")
        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)
        for key, value in metrics.items():
            self.log(f"val_{key}", value, on_step=False, on_epoch=True, prog_bar=True)

    def configure_optimizers(self):
        optimizer = optim.AdamW(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)
        return optimizer

def parse_args():
    parser = argparse.ArgumentParser(description="Train the ImageBind model with PyTorch Lightning and LoRA.")
    parser.add_argument("--seed", type=int, default=43, help="Random seed for reproducibility")
    parser.add_argument("--device", type=str, default="cpu", help="Device to use for training ('cpu' or 'cuda')")
    parser.add_argument("--datasets_dir", type=str, default="./Dataset",
                        help="Directory containing the datasets")
    parser.add_argument("--datasets", type=str, nargs="+", default=["patientsdump"], choices=["patientsdump"],
                        help="Datasets to use for training and validation")
    parser.add_argument("--full_model_checkpoint_dir", type=str, default=".checkpoints/full",
                        help="Directory to save the full model checkpoints")
    parser.add_argument("--full_model_checkpointing", action="store_true", help="Save full model checkpoints")
    parser.add_argument("--loggers", type=str, nargs="+", choices=["tensorboard", "wandb", "comet", "mlflow"],
                        help="Loggers to use for logging")
    parser.add_argument("--loggers_dir", type=str, default="./.logs", help="Directory to save the logs")
    parser.add_argument("--headless", action="store_true", help="Run in headless mode (Don't plot samples on start)")

    parser.add_argument("--max_epochs", type=int, default=500, help="Maximum number of epochs to train")
    parser.add_argument("--batch_size", type=int, default=12, help="Batch size for training and validation")
    parser.add_argument("--lr", type=float, default=5e-6, help="Learning rate")
    parser.add_argument("--weight_decay", type=float, default=1e-4, help="Weight decay")
    parser.add_argument("--momentum_betas", nargs=2, type=float, default=[0.9, 0.95],
                        help="Momentum beta 1 and 2 for Adam optimizer")
    parser.add_argument("--gradient_clip_val", type=float, default=1.0, help="Gradient clipping value")
    parser.add_argument("--temperature", type=float, default=0.07, help="Temperature parameter for InfoNCE loss")
    parser.add_argument("--num_workers", type=int, default=0, help="Number of workers for data loading")
    parser.add_argument("--self_contrast", action="store_true", help="Use self-contrast on the image modality")

    parser.add_argument("--lora", action="store_true", help="Use LoRA")
    parser.add_argument("--lora_rank", type=int, default=4, help="Rank of LoRA layers")
    parser.add_argument("--lora_checkpoint_dir", type=str, default="./.checkpoints/lora",
                        help="Directory to save LoRA checkpoint")
    parser.add_argument("--lora_modality_names", nargs="+", type=str, default=["vision", "text"],
                        choices=["vision", "text", "audio", "thermal", "depth", "imu"],
                        help="Modality names to apply LoRA")
    parser.add_argument("--lora_layer_idxs", nargs="+", type=int,
                        help="Layer indices to apply LoRA")
    parser.add_argument("--lora_layer_idxs_vision", nargs="+", type=int,
                        help="Layer indices to apply LoRA for vision modality. Overrides lora_layer_idxs if specified")
    parser.add_argument("--lora_layer_idxs_text", nargs="+", type=int,
                        help="Layer indices to apply LoRA for text modality. Overrides lora_layer_idxs if specified")
    parser.add_argument("--lora_layer_idxs_audio", nargs="+", type=int,
                        help="Layer indices to apply LoRA for audio modality. Overrides lora_layer_idxs if specified")
    parser.add_argument("--lora_layer_idxs_thermal", nargs="+", type=int,
                        help="Layer indices to apply LoRA for thermal modality. Overrides lora_layer_idxs if specified")
    parser.add_argument("--lora_layer_idxs_depth", nargs="+", type=int,
                        help="Layer indices to apply LoRA for depth modality. Overrides lora_layer_idxs if specified")
    parser.add_argument("--lora_layer_idxs_imu", nargs="+", type=int,
                        help="Layer indices to apply LoRA for imu modality. Overrides lora_layer_idxs if specified")

    parser.add_argument("--linear_probing", action="store_true",
                        help="Freeze model and train the last layers of the head for each modality.")

    return parser.parse_args()

def train_model(args):
    transform = transforms.Compose([
    transforms.RandomResizedCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])

    image_paths, labels, modalities = load_dataset(args.datasets_dir)
    train_image_paths, val_image_paths, train_labels, val_labels, train_modalities, val_modalities = train_test_split(
        image_paths, labels, modalities, test_size=0.2)

    train_dataset = AlzheimerDataset(train_image_paths, train_labels, train_modalities, transform=transform)
    val_dataset = AlzheimerDataset(val_image_paths, val_labels, val_modalities, transform=transform)

   # train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, #num_workers=args.num_workers)
    train_sampler = BalancedModalityConditionSamplerWithRepetition(train_dataset, batch_size=12)  #   Ensure batch_size is divisible by 4
    train_loader = DataLoader(train_dataset, batch_sampler=train_sampler)

    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers)

    lora_layer_idxs = {}
    lora_modality_names = []
    modalities = ["vision", "text", "audio", "thermal", "depth", "imu"]
    for modality_name in args.lora_modality_names:
        if modality_name in modalities:
            modality_type = getattr(ModalityType, modality_name.upper())
            lora_layer_idxs[modality_type] = getattr(args, f'lora_layer_idxs_{modality_name}', None)
            if not lora_layer_idxs[modality_type]:
                lora_layer_idxs[modality_type] = None
            lora_modality_names.append(modality_type)
        else:
            raise ValueError(f"Unknown modality name: {modality_name}")

    model = ImageBindTrain(lora_rank=args.lora_rank, lora_checkpoint_dir=args.lora_checkpoint_dir, lora=args.lora,
                           num_classes=2, lr=args.lr, weight_decay=args.weight_decay, temperature=args.temperature)

    loggers = []
    if 'tensorboard' in args.loggers:
        loggers.append(TensorBoardLogger("tb_logs", name="my_model"))
    if 'comet' in args.loggers:
        loggers.append(CometLogger(
            api_key="TL29lVHGhSIwuQXeKLmBgZgak",
            workspace="sripad-tech",
            project_name="general",
        ))

    checkpoint_callback = ModelCheckpoint(
        monitor='val_loss_classification',
        dirpath='.checkpoints/full',
        filename='{epoch}-{val_loss_classification:.8f}',
        save_top_k=3,mode='min',
    )

    trainer = pl.Trainer(
        max_epochs=args.max_epochs,
        accelerator='gpu' if torch.cuda.is_available() and 'cuda' in args.device else "cpu",
        devices=1 if torch.cuda.is_available() and "cuda" in args.device else None,
        logger=loggers if loggers else None,
        callbacks=[checkpoint_callback],
    )

    trainer.fit(model, train_loader, val_loader)

if __name__ == "__main__":
    args = parse_args()
    train_model(args)
